## Copyright (C) 2010, Paco Nathan. This work is licensed under 
## the BSD License. To view a copy of this license, visit:
##    http://creativecommons.org/licenses/BSD/
## or send a letter to:
##    Creative Commons, 171 Second Street, Suite 300
##    San Francisco, California, 94105, USA
##
## @author Paco Nathan <ceteri@gmail.com>


Slinky provides an open source, high-performance Web Crawler, plus
common Text Analytics, implemented in Python. 

  * uses Redis key/value store for both CrawlQueue and PageStore
  * uses SQLite to persist crawled URI content
  * uses neo4j to persist crawled URI metadata
  * uses Hadoop, R, Gephi for Text Analytics and Link Analytics

In contrast to a pure MapReduce approach, this leverages a "particle
cluster" design pattern, which is particularly well-suited for
combinging highly reliable servers plus low-cost/unreliable VMs. For
example in AWS, the key/value store could run on a large EC2 node,
while the distributed tasks run on Spot Instances -- based on pricing
and availability. This pattern helps maximize throughput and
reliability while minimizing the cost of scale-out for long-running
jobs.

Requires:
	http://github.com/andymccurdy/redis-py
	http://www.crummy.com/software/BeautifulSoup/
	http://henry.precheur.org/python/rfc3339
	http://www.sqlite.org/download.html

Usage:
	# initialize CrawlQueue and PageStore; run from any node...
	./src/slinky.py redis_host:port:db flush
	./src/slinky.py redis_host:port:db config < config.tsv
	./src/slinky.py redis_host:port:db whitelist < whitelist.tsv
	./src/slinky.py redis_host:port:db seed < urls.tsv

	# perform a crawl; run this on each worker node...
	./src/slinky.py redis_host:port:db perform

	# persist the crawled content; run this on Redis server...
	./src/slinky.py redis_host:port:db persist

	# run MapReduce mapper on URI list for Analytics
	./src/slinky.py redis_host:port:db mapper
